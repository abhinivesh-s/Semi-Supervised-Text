{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM, CNN with GloVE on all DataSets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMR52DBvjqUTnqt9Jgy8cjV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinivesh-s/Semi-Supervised-Text/blob/main/LSTM%2C_CNN_with_GloVE_on_all_DataSets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDiOFPhZQRT8"
      },
      "source": [
        "# IMDB\n",
        "\n",
        "\n",
        "1.   MLP\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueImsZ8KKQKo"
      },
      "source": [
        "!pip install --upgrade scikit-learn  # Do this to use sklearn SelfTrainingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7epwVF9xRFok"
      },
      "source": [
        "!pip install numpy==1.16.2\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_pmDcpcfyaM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data as Data\n",
        "import pickle\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UimuIHXSm1sc"
      },
      "source": [
        "try:\n",
        "    import scikeras\n",
        "except ImportError:\n",
        "    !python -m pip install scikeras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CINtLiQgPlK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa3rhWCfRR_v"
      },
      "source": [
        "print(np.__version__)\n",
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMut3gWbQUxB"
      },
      "source": [
        "max_words = 30000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12jLs2ghs3Wm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feLsjKVQg-mk"
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/MixText-master\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMUWFZuielQN"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# SENT_DETECTOR = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "def train_val_split(labels, n_labeled_per_class, unlabeled_per_class, n_labels, seed=0):\n",
        "    \"\"\"Split the original training set into labeled training set, unlabeled training set, development set\n",
        "\n",
        "    Arguments:\n",
        "        labels {list} -- List of labeles for original training set\n",
        "        n_labeled_per_class {int} -- Number of labeled data per class\n",
        "        unlabeled_per_class {int} -- Number of unlabeled data per class\n",
        "        n_labels {int} -- The number of classes\n",
        "\n",
        "    Keyword Arguments:\n",
        "        seed {int} -- [random seed of np.shuffle] (default: {0})\n",
        "\n",
        "    Returns:\n",
        "        [list] -- idx for labeled training set, unlabeled training set, development set\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    labels = np.array(labels)\n",
        "    train_labeled_idxs = []\n",
        "    train_unlabeled_idxs = []\n",
        "    val_idxs = []\n",
        "\n",
        "    for i in range(n_labels):\n",
        "        idxs = np.where(labels == i)[0]\n",
        "        np.random.shuffle(idxs)\n",
        "        if n_labels == 2:\n",
        "            # IMDB\n",
        "            \n",
        "            \n",
        "            \n",
        "            n_unlabeled_per_class = unlabeled_per_class   #10, 100, 500, 1000, 2500\n",
        "            train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            train_unlabeled_idxs.extend(idxs[500: 500 + n_unlabeled_per_class])\n",
        "            val_idxs.extend(idxs[-2000:])\n",
        "        \n",
        "        \n",
        "        \n",
        "            # train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            # train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            # train_unlabeled_idxs.extend(\n",
        "            #     idxs[500: 500 + 5000])\n",
        "            # val_idxs.extend(idxs[-2000:])\n",
        "            \n",
        "\n",
        "    return train_labeled_idxs, train_unlabeled_idxs, val_idxs\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "#Removes Punctuations\n",
        "def remove_punctuations(data):\n",
        "    punct_tag=re.compile(r'[^\\w\\s]')\n",
        "    data=punct_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes HTML syntaxes\n",
        "def remove_html(data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes URL data\n",
        "def remove_url(data):\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def clean(train_df):\n",
        "    \n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_html(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_url(z))\n",
        "    # train_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\n",
        "    \n",
        "    train_df['review']=train_df['review'].apply(lambda z: word_tokenize(z))\n",
        "    \n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # train_df['review']=train_df['review'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "    train_df['review']=train_df['review'].apply(lambda x: ' '.join(x))\n",
        "    \n",
        "    return train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6GvmQwA0-gz"
      },
      "source": [
        "#------------------For GloVE---------------------#\n",
        "embedding_dim = 300\n",
        "maxlen = 500\n",
        "\n",
        "\n",
        "def get_embed_matrix(word_index):\n",
        "\n",
        "    hits = 0\n",
        "    misses = 0\n",
        "\n",
        "    hit_words = []\n",
        "    missed_words = []\n",
        "    # Prepare embedding matrix\n",
        "    \n",
        "\n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (max_words, 100)\n",
        "    for word, i in word_index.items():\n",
        "        if i < max_words:\n",
        "            embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros\n",
        "                hits += 1\n",
        "                hit_words.append(word)\n",
        "            else:\n",
        "                misses += 1\n",
        "                missed_words.append(word)  \n",
        "\n",
        "\n",
        "                \n",
        "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "    print(\"Missed words: \", missed_words)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "    #------------------For GloVE---------------------#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrxr2IasgkE6"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "def create_model(word_index):\n",
        "\n",
        "    # int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    # embedded_sequences = embedding_layer(int_sequences_input)\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(max_words, embedding_dim, input_length = maxlen),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(100, activation='relu'),\n",
        "        tf.keras.layers.Dense(50),\n",
        "    ])\n",
        "\n",
        "    embedding_matrix = get_embed_matrix(word_index)\n",
        "    model.layers[0].set_weights([embedding_matrix])\n",
        "    model.layers[0].trainable = False\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  # loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "def create_model_lstm(word_index, num_labels):\n",
        "\n",
        "    # int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    # embedded_sequences = embedding_layer(int_sequences_input)\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(max_words, embedding_dim, input_length = maxlen),\n",
        "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "        tf.keras.layers.Dropout(0.6),\n",
        "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "        tf.keras.layers.Dropout(0.6),\n",
        "        tf.keras.layers.LSTM(128),             \n",
        "        # tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(100, activation='relu'),\n",
        "        tf.keras.layers.Dense(num_labels),\n",
        "    ])\n",
        "\n",
        "    embedding_matrix = get_embed_matrix(word_index)\n",
        "    model.layers[0].set_weights([embedding_matrix])\n",
        "    model.layers[0].trainable = False\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  # loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LMrfD2YNXZ-"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.engine.topology import Input\n",
        "from keras.layers import MaxPooling1D,GlobalMaxPooling1D,GlobalAveragePooling1D,Conv1D,Dense,Dropout\n",
        "from keras import regularizers\n",
        "from keras.models import Model,load_model\n",
        "from keras import optimizers\n",
        "import h5py # necessary for saving keras model\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score,classification_report\n",
        "from keras.layers import Concatenate,GaussianNoise\n",
        "\n",
        "from scikeras.wrappers import KerasClassifier as KC_updated\n",
        "\n",
        "\n",
        "def create_model_cnn(word_index, num_labels):\n",
        "\n",
        "    embedding_matrix = get_embed_matrix(word_index)\n",
        "    # model.layers[0].set_weights([embedding_matrix])\n",
        "    # model.layers[0].trainable = False\n",
        "\n",
        "    embedding_layer_d = Embedding(max_words,embedding_dim,weights = [embedding_matrix],input_length = maxlen,trainable = False)\n",
        "\n",
        "    # int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    # embedded_sequences = embedding_layer(int_sequences_input)\n",
        "\n",
        "\n",
        "    # model = tf.keras.Sequential([\n",
        "    #     tf.keras.layers.Embedding(max_words, embedding_dim, input_length = maxlen),\n",
        "    #     tf.keras.layers.LSTM(64),\n",
        "    #     # tf.keras.layers.Flatten(),\n",
        "    #     tf.keras.layers.Dense(100, activation='relu'),\n",
        "    #     tf.keras.layers.Dense(50),\n",
        "    # ])\n",
        "\n",
        "    sequence_input = Input(shape = (maxlen,),dtype = 'int32')\n",
        "    embedded_input = embedding_layer_d(sequence_input)\n",
        "\n",
        "    input_x = GaussianNoise(stddev = 0.01)(embedded_input)\n",
        "\n",
        "    x1 = Conv1D(128,2,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "\n",
        "    x2 = Conv1D(128,3,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
        "    x2 = Dropout(0.5)(x2)\n",
        "\n",
        "    x3 = Conv1D(128,4,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
        "    x3 = Dropout(0.5)(x3)\n",
        "\n",
        "    x4 = Conv1D(128,5,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
        "    x4 = Dropout(0.5)(x4)\n",
        "\n",
        "    x11 = GlobalMaxPooling1D()(x1)# 128 dim vector\n",
        "    x21 = GlobalMaxPooling1D()(x2) # 128 dim vector\n",
        "    x31 = GlobalMaxPooling1D()(x3) # 128 dim vector\n",
        "    x41 = GlobalMaxPooling1D()(x4) # 128 dim vector\n",
        "\n",
        "    x12 = GlobalAveragePooling1D()(x1)# 128 dim vector\n",
        "    x22 = GlobalAveragePooling1D()(x2) # 128 dim vector\n",
        "    x32 = GlobalAveragePooling1D()(x3) # 128 dim vector\n",
        "    x42 = GlobalAveragePooling1D()(x4) # 128 dim vector\n",
        "\n",
        "    x = Concatenate(axis=-1)([x11,x12,x21,x22,x31,x32,x41,x42])\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    preds = Dense(num_labels, kernel_regularizer = regularizers.l2(1e-5))(x)\n",
        "\n",
        "    model_d = Model(sequence_input,preds)\n",
        "    optimizer = optimizers.RMSprop(learning_rate=0.001, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
        "    model_d.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    ,optimizer=optimizer,metrics=['acc'])\n",
        "\n",
        "\n",
        "\n",
        "    # model.compile(optimizer='adam',\n",
        "    #               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    #               # loss='categorical_crossentropy',\n",
        "    #               metrics=['accuracy'])\n",
        "    \n",
        "    return model_d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ8OCqGFirni"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Parameters\n",
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "# Supervised Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(**sdg_params)),\n",
        "])\n",
        "\n",
        "# # Supervised Pipeline\n",
        "# pipeline = Pipeline([\n",
        "#     ('vect', CountVectorizer(**vectorizer_params)),\n",
        "#     ('tfidf', TfidfTransformer()),\n",
        "#     ('clf', SVC(probability=True, gamma=\"auto\")),\n",
        "# ])\n",
        "\n",
        "# SelfTraining Pipeline\n",
        "st_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True)),\n",
        "])\n",
        "\n",
        "# # SelfTraining Pipeline\n",
        "# st_pipeline = Pipeline([\n",
        "#     ('vect', CountVectorizer(**vectorizer_params)),\n",
        "#     ('tfidf', TfidfTransformer()),\n",
        "#     ('clf', SelfTrainingClassifier(SVC(probability=True, gamma=\"auto\") )),\n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# LabelSpreading Pipeline\n",
        "ls_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    # LabelSpreading does not support dense matrices\n",
        "    ('todense', FunctionTransformer(lambda x: x.todense())),\n",
        "    ('clf', LabelSpreading()),\n",
        "])\n",
        "\n",
        "\n",
        "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    \n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # if -1 in y_train:\n",
        "    #   print(\"Y-PRED-PROBA\", clf.predict_proba(X_train))\n",
        "\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "def eval_and_print_metrics_df(clf, X_train, y_train, X_test, y_test, thresh = None, kbest = None):\n",
        "\n",
        "    dict1 = {}\n",
        "\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    \n",
        "    dict1['Labeled'] = len(X_train) - sum(1 for x in y_train if x == -1)\n",
        "    dict1['UnLabeled'] = sum(1 for x in y_train if x == -1)\n",
        "\n",
        "\n",
        "       \n",
        "    # if sum(1 for x in y_train if x == -1) == 0:\n",
        "    #     dict1['type'] = 'Supervised'\n",
        "    # else:\n",
        "    #     dict1['type'] = 'Semi-Supervised'\n",
        "\n",
        "    dict1['Threshold'] = thresh\n",
        "    dict1['K-Best'] = kbest\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    # print('y_train:', y_train)\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      clf.fit(X_train, y_train)\n",
        "      y_pred = np.array(clf.predict(X_test))\n",
        "      \n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "    dict1['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    dict1['roc-auc'] = roc_auc_score(y_test, y_pred)\n",
        "    dict1['precision'] = precision_score(y_test, y_pred)\n",
        "    dict1['recall'] = recall_score(y_test, y_pred)\n",
        "    dict1['f1'] = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "    return dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM1anuRRbDs3"
      },
      "source": [
        "%%time\n",
        "path_to_glove_file =  \"glove.840B.\"+str(embedding_dim)+\"d.txt\"\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "      values = line.split(' ')\n",
        "      word = values[0] ## The first entry is the word\n",
        "      coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
        "      embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV_Wn4hb11_j"
      },
      "source": [
        "# %%time\n",
        "# path_to_glove_file =  \"glove.6B.\"+str(embedding_dim)+\"d.txt\"\n",
        "# embeddings_index = {}\n",
        "# with open(path_to_glove_file) as f:\n",
        "#     for line in f:\n",
        "#         word, coefs = line.split(maxsplit=1)\n",
        "#         coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "#         embeddings_index[word] = coefs\n",
        "\n",
        "# print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbjwldGL7gyw"
      },
      "source": [
        "#MLP for IMDB\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing. sequence import pad_sequences\n",
        "\n",
        "\n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=50,activation = 'relu',solver='adam',random_state=1)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, max_features=30000)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    n_list = [1000]\n",
        "    kbest_list=[200, 500, 750, 1000, 2000]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      tf.keras.backend.clear_session()\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels  \n",
        "\n",
        "      tokenizer = Tokenizer(num_words=max_words)\n",
        "      tokenizer.fit_on_texts(np.concatenate((X_train, X_test)))\n",
        "      sequences = tokenizer.texts_to_sequences(np.concatenate((X_train, X_test)))\n",
        "      word_index = tokenizer.word_index                   \n",
        "      print(\"Found %s unique tokens.\" % len(word_index)) \n",
        "\n",
        "      data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "      X_train, X_test = data[:len(X_train)], data[len(X_train):]\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      # print(\"Shape of X_test: \", len(X_test)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # print(\"Y-TYPE: \", type(labels[0]))  \n",
        "      \n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = X_train[:2*n], labels[:2*n]\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # y_test = to_categorical(y_test)\n",
        "      # y_20 = to_categorical(y_20)\n",
        "      # y_test = to_categorical(y_test)\n",
        "      \n",
        "\n",
        "      model = KerasClassifier(build_fn=lambda: create_model(word_index), epochs=20, verbose=0)\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          # ('tfidf', TfidfTransformer()), \n",
        "          # # ('kbest', MySelectKBest(f_classif, k=20000)),         \n",
        "          # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "          ('clf', model)\n",
        "      ])\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for k in kbest_list:\n",
        "        print(\"---------------------------------Threshold = \", k,\"---------------------------------\")\n",
        "        tf.keras.backend.clear_session()\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        model = KerasClassifier(build_fn=lambda: create_model(word_index), epochs=20, verbose=0)\n",
        "        st_pipeline = Pipeline([\n",
        "            # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            # ('tfidf', TfidfTransformer()),\n",
        "            # # ('kbest', MySelectKBest(f_classif, k=20000)),\n",
        "            # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "            ('clf', SelfTrainingClassifier(model, criterion = 'k_best', k_best = k, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = None, kbest = k)\n",
        "        df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9sasRxU72OG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qET_kd-Y0_Sk"
      },
      "source": [
        "#LSTM for IMDB\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing. sequence import pad_sequences\n",
        "\n",
        "\n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=50,activation = 'relu',solver='adam',random_state=1)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, max_features=30000)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[200, 500, 750, 1000, 2000]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      tf.keras.backend.clear_session()\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels  \n",
        "\n",
        "      tokenizer = Tokenizer(num_words=max_words)\n",
        "      tokenizer.fit_on_texts(np.concatenate((X_train, X_test)))\n",
        "      sequences = tokenizer.texts_to_sequences(np.concatenate((X_train, X_test)))\n",
        "      word_index = tokenizer.word_index                   \n",
        "      print(\"Found %s unique tokens.\" % len(word_index)) \n",
        "\n",
        "      data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "      X_train, X_test = data[:len(X_train)], data[len(X_train):]\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      # print(\"Shape of X_test: \", len(X_test)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # print(\"Y-TYPE: \", type(labels[0]))  \n",
        "      \n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = X_train[:2*n], labels[:2*n]\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # y_test = to_categorical(y_test)\n",
        "      # y_20 = to_categorical(y_20)\n",
        "      # y_test = to_categorical(y_test)\n",
        "      \n",
        "\n",
        "      model = KC_updated(model=lambda: create_model_lstm(word_index, 2), epochs=20, verbose=0)\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          # ('tfidf', TfidfTransformer()), \n",
        "          # # ('kbest', MySelectKBest(f_classif, k=20000)),         \n",
        "          # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "          ('clf', model)\n",
        "      ])\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for k in kbest_list:\n",
        "        print(\"---------------------------------Threshold = \", k,\"---------------------------------\")\n",
        "        tf.keras.backend.clear_session()\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        model = KC_updated(model=lambda: create_model_lstm(word_index, 2), epochs=20, verbose=0)\n",
        "        st_pipeline = Pipeline([\n",
        "            # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            # ('tfidf', TfidfTransformer()),\n",
        "            # # ('kbest', MySelectKBest(f_classif, k=20000)),\n",
        "            # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "            ('clf', SelfTrainingClassifier(model, criterion = 'k_best', k_best = k, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = None, kbest = k)\n",
        "        df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJZCKFRX1AQ9"
      },
      "source": [
        "df_mlp_imdb_2k.to_excel('tf-lstm_imdb_GloVE_840.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffI2ALf-PnrH"
      },
      "source": [
        "#CNN for IMDB\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing. sequence import pad_sequences\n",
        "\n",
        "\n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=50,activation = 'relu',solver='adam',random_state=1)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, max_features=30000)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[200, 500, 750, 1000, 2000]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      tf.keras.backend.clear_session()\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels  \n",
        "\n",
        "      tokenizer = Tokenizer(num_words=max_words)\n",
        "      tokenizer.fit_on_texts(np.concatenate((X_train, X_test)))\n",
        "      sequences = tokenizer.texts_to_sequences(np.concatenate((X_train, X_test)))\n",
        "      word_index = tokenizer.word_index                   \n",
        "      print(\"Found %s unique tokens.\" % len(word_index)) \n",
        "\n",
        "      data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "      X_train, X_test = data[:len(X_train)], data[len(X_train):]\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      # print(\"Shape of X_test: \", len(X_test)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # print(\"Y-TYPE: \", type(labels[0]))  \n",
        "      \n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = X_train[:2*n], labels[:2*n]\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # y_test = to_categorical(y_test)\n",
        "      # y_20 = to_categorical(y_20)\n",
        "      # y_test = to_categorical(y_test)\n",
        "      \n",
        "\n",
        "      model = KC_updated(model=lambda: create_model_cnn(word_index, 2), epochs=20, verbose=0)\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          # ('tfidf', TfidfTransformer()), \n",
        "          # # ('kbest', MySelectKBest(f_classif, k=20000)),         \n",
        "          # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "          ('clf', model)\n",
        "      ])\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for k in kbest_list:\n",
        "        print(\"---------------------------------Threshold = \", k,\"---------------------------------\")\n",
        "        tf.keras.backend.clear_session()\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        model = KC_updated(model=lambda: create_model_cnn(word_index, 2), epochs=20, verbose=0)\n",
        "        st_pipeline = Pipeline([\n",
        "            # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            # ('tfidf', TfidfTransformer()),\n",
        "            # # ('kbest', MySelectKBest(f_classif, k=20000)),\n",
        "            # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "            ('clf', SelfTrainingClassifier(model, criterion = 'k_best', k_best = k, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = None, kbest = k)\n",
        "        df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k6zBNXKsCdt"
      },
      "source": [
        "df_mlp_imdb_2k.to_excel('tf-cnn_imdb_GloVE_non-trainable_840.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJbPC72BjeZ4"
      },
      "source": [
        "#LSTM for IMDB\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing. sequence import pad_sequences\n",
        "\n",
        "\n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=50,activation = 'relu',solver='adam',random_state=1)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, max_features=30000)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      tf.keras.backend.clear_session()\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels  \n",
        "\n",
        "      tokenizer = Tokenizer(num_words=max_words)\n",
        "      tokenizer.fit_on_texts(np.concatenate((X_train, X_test)))\n",
        "      sequences = tokenizer.texts_to_sequences(np.concatenate((X_train, X_test)))\n",
        "      word_index = tokenizer.word_index                   \n",
        "      print(\"Found %s unique tokens.\" % len(word_index)) \n",
        "\n",
        "      data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "      X_train, X_test = data[:len(X_train)], data[len(X_train):]\n",
        "\n",
        "      # print(\"Shape of X_test: \", len(X_test)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # print(\"Y-TYPE: \", type(labels[0]))  \n",
        "      \n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = X_train[:2*n], labels[:2*n]\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      model = KerasClassifier(build_fn=lambda: create_model_lstm(word_index), epochs=20, verbose=0)\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          # ('tfidf', TfidfTransformer()), \n",
        "          # # ('kbest', MySelectKBest(f_classif, k=20000)),         \n",
        "          # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "          ('clf', model)\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "        tf.keras.backend.clear_session()\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        model = KerasClassifier(build_fn=lambda: create_model_lstm(word_index), epochs=20, verbose=0)\n",
        "        st_pipeline = Pipeline([\n",
        "            # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            # ('tfidf', TfidfTransformer()),\n",
        "            # # ('kbest', MySelectKBest(f_classif, k=20000)),\n",
        "            # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "            ('clf', SelfTrainingClassifier(model, criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29Gz04HjrZh8"
      },
      "source": [
        "df_mlp_imdb_2k.to_excel('tf-lstm_imdb_GloVE_840.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S93ITjm-Vizu"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRm3W-w9ulfI"
      },
      "source": [
        "# !wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "# !unzip -q glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6vaKQ5uVjA6"
      },
      "source": [
        "nn\n",
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "hit_words = []\n",
        "missed_words = []\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "        hit_words.append(word)\n",
        "    else:\n",
        "        misses += 1\n",
        "        missed_words.append(word)\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "print(\"Missed words: \", missed_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwbyekEoVjHu"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIrv-WImVjOm"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1FgEh75VjWB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2IQ6HAHVjci"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a9w4xkcVjjI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9RgAvBEVjp_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrSNg00Xmm-1"
      },
      "source": [
        "# NewsGroup\n",
        "\n",
        "\n",
        "1.   LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFLJW6Dtmn-4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuaecprcLFwE"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "data = fetch_20newsgroups(subset='train', categories=None)\n",
        "print(\"%d documents\" % len(data.filenames))\n",
        "print(\"%d categories\" % len(data.target_names))\n",
        "print()\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "def eval_and_print_metrics_df(clf, X_train, y_train, X_test, y_test, thresh = None, kbest = None):\n",
        "\n",
        "    dict1 = {}\n",
        "\n",
        "\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    \n",
        "    dict1['Labeled'] = len(X_train) - sum(1 for x in y_train if x == -1)\n",
        "    dict1['UnLabeled'] = sum(1 for x in y_train if x == -1)\n",
        "\n",
        "\n",
        "    \n",
        "    # if sum(1 for x in y_train if x == -1) == 0:\n",
        "    #     dict1['type'] = 'Supervised'\n",
        "    # else:\n",
        "    #     dict1['type'] = 'Semi-Supervised'\n",
        "\n",
        "    dict1['Threshold'] = thresh\n",
        "    dict1['K-Best'] = kbest\n",
        "\n",
        "    # print('y_train:', y_train)\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      clf.fit(X_train, y_train)\n",
        "      y_pred = np.array(clf.predict(X_test))\n",
        "      \n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "    dict1['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    # dict1['roc-auc'] = roc_auc_score(y_test, y_pred, multi_class=\"ovr\")\n",
        "    dict1['precision'] = precision_score(y_test, y_pred, average='macro')\n",
        "    dict1['recall'] = recall_score(y_test, y_pred, average='macro')\n",
        "    dict1['f1'] = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "    return dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0qh5Wo7ppIM"
      },
      "source": [
        "#------------------For GloVE---------------------#\n",
        "embedding_dim = 300\n",
        "maxlen = 500\n",
        "\n",
        "\n",
        "def get_embed_matrix(word_index):\n",
        "\n",
        "    hits = 0\n",
        "    misses = 0\n",
        "\n",
        "    hit_words = []\n",
        "    missed_words = []\n",
        "    # Prepare embedding matrix\n",
        "    \n",
        "\n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (max_words, 100)\n",
        "    for word, i in word_index.items():\n",
        "        if i < max_words:\n",
        "            embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros\n",
        "                hits += 1\n",
        "                hit_words.append(word)\n",
        "            else:\n",
        "                misses += 1\n",
        "                missed_words.append(word)  \n",
        "\n",
        "\n",
        "                \n",
        "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "    print(\"Missed words: \", missed_words)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "    #------------------For GloVE---------------------#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agoXVj3bbmKv"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "def create_model(word_index):\n",
        "\n",
        "    # int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    # embedded_sequences = embedding_layer(int_sequences_input)\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(max_words, embedding_dim, input_length = maxlen),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(100, activation='relu'),\n",
        "        tf.keras.layers.Dense(50),\n",
        "    ])\n",
        "\n",
        "    embedding_matrix = get_embed_matrix(word_index)\n",
        "    model.layers[0].set_weights([embedding_matrix])\n",
        "    model.layers[0].trainable = False\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  # loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoeZy4Br3xYO"
      },
      "source": [
        "# def vectorize_data(data, vocab: dict) -> list:\n",
        "#     keys = list(vocab.keys())\n",
        "#     filter_unknown = lambda word: vocab.get(word, None) is not None\n",
        "#     encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
        "#     vectorized = list(map(encode, data))\n",
        "#     print('Vectorize sentences... (done)')\n",
        "#     return vectorized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZQqQjtb6N45"
      },
      "source": [
        "%%time\n",
        "# MLP for NG\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Tools for creating ngrams and vectorizing input data\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Parameters\n",
        "# mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=100,activation = 'relu',solver='adam',random_state=1,learning_rate_init=0.01,\n",
        "#                   learning_rate='adaptive')\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, max_features=30000)\n",
        "\n",
        "\n",
        "df_mlp_ng = pd.DataFrame()\n",
        "\n",
        "n_list = [10, 20, 30, 40, 50]\n",
        "kbest_list=[4, 5, 6, 7, 8]\n",
        "threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "\n",
        "X, y = data.data, data.target\n",
        "# embedding_vector_size = 256\n",
        "# trigrams_model = Word2Vec(sentences = X, size = embedding_vector_size, min_count=3, window=5, workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVNMNhGd4Hjg"
      },
      "source": [
        "%%time\n",
        "path_to_glove_file =  \"glove.840B.\"+str(embedding_dim)+\"d.txt\"\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "      values = line.split(' ')\n",
        "      word = values[0] ## The first entry is the word\n",
        "      coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
        "      embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5XhzlObpBpB"
      },
      "source": [
        "# %%time\n",
        "# path_to_glove_file =  \"glove.6B.\"+str(embedding_dim)+\"d.txt\"\n",
        "# embeddings_index = {}\n",
        "# with open(path_to_glove_file) as f:\n",
        "#     for line in f:\n",
        "#         word, coefs = line.split(maxsplit=1)\n",
        "#         coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "#         embeddings_index[word] = coefs\n",
        "\n",
        "# print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRm3jB7f_icp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt36aMsq4ipJ"
      },
      "source": [
        "# CNN for NG\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing. sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=50,activation = 'relu',solver='adam',random_state=1)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, max_features=30000)\n",
        "\n",
        "kbest_list = [400, 1000, 1500, 2000, 4000]\n",
        "\n",
        "n_list = [2.5]\n",
        "threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:    \n",
        "  tf.keras.backend.clear_session()  \n",
        "  print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "  # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "  # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "  np.random.seed(0)\n",
        "  # print(\"Shape before: \", len(X_train), len(X_test))\n",
        "\n",
        "  X_train = np.array([i for i in X_train])\n",
        "  X_test = np.array([i for i in X_test])\n",
        "\n",
        "  # print(\"Shape middle: \", X_train.shape, X_test.shape)\n",
        "  # print(X_train[0])\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=max_words)\n",
        "  tokenizer.fit_on_texts(np.concatenate((X_train, X_test)))\n",
        "  sequences = tokenizer.texts_to_sequences(np.concatenate((X_train, X_test)))\n",
        "  word_index = tokenizer.word_index                   \n",
        "  print(\"Found %s unique tokens.\" % len(word_index)) \n",
        "\n",
        "  data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "  X_train, X_test = data[:len(X_train)], data[len(X_train):]\n",
        "\n",
        "  # print(\"Shape after: \", X_train.shape, X_test.shape)\n",
        "  # print(X_train[0])\n",
        "\n",
        "\n",
        "\n",
        "  unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "  X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                  for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "  \n",
        "  X_u50 = np.array(X_u50)\n",
        "  y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "  X_50, y_50 = map(list, zip(*((x, y)\n",
        "            for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "  X_50 = np.array(X_50)\n",
        "  y_50 = np.array(y_50)\n",
        "\n",
        "\n",
        "  percentage = 2*(n/100)\n",
        "  y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "  # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "  X_20, y_20 = map(list, zip(*((x, y)\n",
        "                  for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "  X_20 = np.array(X_20)\n",
        "  y_20 = np.array(y_20)\n",
        "\n",
        "  print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "  # print(\"Shape after after: \", X_20.shape)\n",
        "  # break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # print(len(X_20),\"SHAPE\")\n",
        "\n",
        "\n",
        "  model = KC_updated(model=lambda: create_model_cnn(word_index, 20), epochs=20, verbose=0)\n",
        "  pipeline = Pipeline([\n",
        "      # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "      # ('tfidf', TfidfTransformer()), \n",
        "      # # ('kbest', MySelectKBest(f_classif, k=20000)),         \n",
        "      # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "      ('clf', model)\n",
        "  ])\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "  df_mlp_ng = df_mlp_ng.append(temp, ignore_index=True)\n",
        "\n",
        "  # set the non-masked subset to be unlabeled\n",
        "  # set only 50% of data to be unlabeled in every iteration of training.\n",
        "  print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "        \"is unlabeled):\")\n",
        "  for k in kbest_list:\n",
        "    print(\"---------------------------------Threshold = \", k,\"---------------------------------\")\n",
        "    tf.keras.backend.clear_session()\n",
        "  \n",
        "  # X_50, y_50 = map(list, zip(*((x, y)\n",
        "  #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "    # SelfTraining Pipeline\n",
        "    model = KC_updated(model=lambda: create_model_cnn(word_index, 20), epochs=20, verbose=0)\n",
        "    st_pipeline = Pipeline([\n",
        "        # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "        # ('tfidf', TfidfTransformer()),\n",
        "        # # ('kbest', MySelectKBest(f_classif, k=20000)),\n",
        "        # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "        ('clf', SelfTrainingClassifier(model, criterion = 'k_best', k_best = k, verbose=True)),\n",
        "    ])\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    temp = eval_and_print_metrics_df(st_pipeline, np.concatenate((X_20,X_u50)), np.concatenate((y_20, y_u50)), X_test, y_test, thresh = None, kbest = k)\n",
        "    df_mlp_ng = df_mlp_ng.append(temp, ignore_index=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqo_VfJY4nYa"
      },
      "source": [
        "df_mlp_ng.to_excel('tf-cnn_ng_GloVE_non-trainable_840_218_posts.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vADSNzoftiZ4"
      },
      "source": [
        "# LSTM for NG\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing. sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=50,activation = 'relu',solver='adam',random_state=1)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, max_features=30000)\n",
        "\n",
        "for n in n_list:    \n",
        "  tf.keras.backend.clear_session()  \n",
        "  print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "  # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "  # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "  np.random.seed(0)\n",
        "  # print(\"Shape before: \", len(X_train), len(X_test))\n",
        "\n",
        "  X_train = np.array([i for i in X_train])\n",
        "  X_test = np.array([i for i in X_test])\n",
        "\n",
        "  # print(\"Shape middle: \", X_train.shape, X_test.shape)\n",
        "  # print(X_train[0])\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=max_words)\n",
        "  tokenizer.fit_on_texts(np.concatenate((X_train, X_test)))\n",
        "  sequences = tokenizer.texts_to_sequences(np.concatenate((X_train, X_test)))\n",
        "  word_index = tokenizer.word_index                   \n",
        "  print(\"Found %s unique tokens.\" % len(word_index)) \n",
        "\n",
        "  data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "  X_train, X_test = data[:len(X_train)], data[len(X_train):]\n",
        "\n",
        "  # print(\"Shape after: \", X_train.shape, X_test.shape)\n",
        "  # print(X_train[0])\n",
        "\n",
        "\n",
        "\n",
        "  unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "  X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                  for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "  \n",
        "  X_u50 = np.array(X_u50)\n",
        "  y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "  X_50, y_50 = map(list, zip(*((x, y)\n",
        "            for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "  X_50 = np.array(X_50)\n",
        "  y_50 = np.array(y_50)\n",
        "\n",
        "\n",
        "  percentage = 2*(n/100)\n",
        "  y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "  # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "  X_20, y_20 = map(list, zip(*((x, y)\n",
        "                  for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "  X_20 = np.array(X_20)\n",
        "  y_20 = np.array(y_20)\n",
        "\n",
        "  print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "  # print(\"Shape after after: \", X_20.shape)\n",
        "  # break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # print(len(X_20),\"SHAPE\")\n",
        "\n",
        "\n",
        "  model = KerasClassifier(build_fn=lambda: create_model_lstm(word_index, 20), epochs=20, verbose=0)\n",
        "  pipeline = Pipeline([\n",
        "      # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "      # ('tfidf', TfidfTransformer()), \n",
        "      # # ('kbest', MySelectKBest(f_classif, k=20000)),         \n",
        "      # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "      ('clf', model)\n",
        "  ])\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "  df_mlp_ng = df_mlp_ng.append(temp, ignore_index=True)\n",
        "\n",
        "  # set the non-masked subset to be unlabeled\n",
        "  # set only 50% of data to be unlabeled in every iteration of training.\n",
        "  print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "        \"is unlabeled):\")\n",
        "  for t in threshold:\n",
        "    print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "    tf.keras.backend.clear_session()\n",
        "  \n",
        "  # X_50, y_50 = map(list, zip(*((x, y)\n",
        "  #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "    # SelfTraining Pipeline\n",
        "    model = KerasClassifier(build_fn=lambda: create_model_lstm(word_index, 20), epochs=20, verbose=0)\n",
        "    st_pipeline = Pipeline([\n",
        "        # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "        # ('tfidf', TfidfTransformer()),\n",
        "        # # ('kbest', MySelectKBest(f_classif, k=20000)),\n",
        "        # ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "        ('clf', SelfTrainingClassifier(model, criterion = 'threshold', threshold = t, verbose=True)),\n",
        "    ])\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    temp = eval_and_print_metrics_df(st_pipeline, np.concatenate((X_20,X_u50)), np.concatenate((y_20, y_u50)), X_test, y_test, thresh = t, kbest = None)\n",
        "    df_mlp_ng = df_mlp_ng.append(temp, ignore_index=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ouXUdkUeuL"
      },
      "source": [
        "df_mlp_ng.to_excel('tf-lstm_ng_GloVE_840_non-trainable_840_218_posts.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaY9fKLPu_LI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVGV4uBvBQO"
      },
      "source": [
        "# Enron\n",
        "\n",
        "1. LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DW-Lg9lZPIz"
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/MixText-master\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJG-THubXVaI"
      },
      "source": [
        "import os\n",
        "os.listdir('data/enron/to_use_for_experiments')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-JImObRXG9b"
      },
      "source": [
        "setup_1 = [i for i in os.listdir('data/enron/to_use_for_experiments') if ('setup1_' in i) and ('setup1_m' not in i)]\n",
        "setup_2 = [i for i in os.listdir('data/enron/to_use_for_experiments') if ('setup2_' in i) and ('setup2_m' not in i)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD1NEhJOXp5T"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKaWlLxrXqOg"
      },
      "source": [
        "# Parameters\n",
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFUVzokG4Wk8"
      },
      "source": [
        "%%time\n",
        "path_to_glove_file =  \"glove.840B.\"+str(embedding_dim)+\"d.txt\"\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "      values = line.split(' ')\n",
        "      word = values[0] ## The first entry is the word\n",
        "      coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
        "      embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5qJ2tD56RdF"
      },
      "source": [
        "# %%time\n",
        "# path_to_glove_file =  \"glove.6B.\"+str(embedding_dim)+\"d.txt\"\n",
        "# embeddings_index = {}\n",
        "# with open(path_to_glove_file) as f:\n",
        "#     for line in f:\n",
        "#         word, coefs = line.split(maxsplit=1)\n",
        "#         coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "#         embeddings_index[word] = coefs\n",
        "\n",
        "# print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOHhUqfDXqWp"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#SGD\n",
        "def eval_and_print_metrics_df(clf, X_train, y_train, X_test, y_test, thresh = None, kbest = None, label = None):\n",
        "\n",
        "    dict1 = {}\n",
        "\n",
        "\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    \n",
        "    dict1['Labeled'] = len(X_train) - sum(1 for x in y_train if x == -1)\n",
        "    dict1['UnLabeled'] = sum(1 for x in y_train if x == -1)\n",
        "    dict1['label_used'] = label\n",
        "\n",
        "    tr_0, tr_1 = sum(1 for x in y_train if x == 0), sum(1 for x in y_train if x == 1)\n",
        "    te_0, te_1 = sum(1 for x in y_test if x == 0), sum(1 for x in y_test if x == 1)\n",
        "\n",
        "    dict1['Train 0-1-trivial'] = str(tr_0)+'-'+str(tr_1)+'-'+\\\n",
        "                                                str(max(tr_0, tr_1)/(tr_0+tr_1))\n",
        "\n",
        "    dict1['Test 0-1-trivial'] = str(te_0)+'-'+str(te_1)+'-'+\\\n",
        "                                                str(max(te_0, te_1)/(te_0+te_1))\n",
        "\n",
        "    \n",
        "    # if sum(1 for x in y_train if x == -1) == 0:\n",
        "    #     dict1['type'] = 'Supervised'\n",
        "    # else:\n",
        "    #     dict1['type'] = 'Semi-Supervised'\n",
        "\n",
        "    dict1['Threshold'] = thresh\n",
        "    dict1['K-Best'] = kbest\n",
        "\n",
        "    # print('y_train:', y_train)\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      clf.fit(X_train, y_train)\n",
        "      y_pred = np.array(clf.predict(X_test))\n",
        "      \n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "    dict1['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    dict1['roc-auc'] = roc_auc_score(y_test, y_pred)\n",
        "    dict1['precision'] = precision_score(y_test, y_pred)\n",
        "    dict1['recall'] = recall_score(y_test, y_pred)\n",
        "    dict1['f1'] = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "    return dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHqUNbRE6bAk"
      },
      "source": [
        "#------------------For GloVE---------------------#\n",
        "embedding_dim = 300\n",
        "maxlen = 500\n",
        "\n",
        "\n",
        "def get_embed_matrix(word_index):\n",
        "\n",
        "    hits = 0\n",
        "    misses = 0\n",
        "\n",
        "    hit_words = []\n",
        "    missed_words = []\n",
        "    # Prepare embedding matrix\n",
        "    \n",
        "\n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (max_words, 100)\n",
        "    for word, i in word_index.items():\n",
        "        if i < max_words:\n",
        "            embedding_vector = embeddings_index.get(word) # embedding_vector.shape (100,)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector # Words not found in the mebedding index will all be zeros\n",
        "                hits += 1\n",
        "                hit_words.append(word)\n",
        "            else:\n",
        "                misses += 1\n",
        "                missed_words.append(word)  \n",
        "\n",
        "\n",
        "                \n",
        "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "    print(\"Missed words: \", missed_words)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "    #------------------For GloVE---------------------#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNNMuald5-YM"
      },
      "source": [
        "# CNN for Enron.\n",
        "kbest_list=[200, 500, 750, 1000, 2000]\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "    df_sgd_en = pd.DataFrame()\n",
        "\n",
        "    path = 'data/enron/main'\n",
        "    main_labels = ['message', 'subject', 'body', 'full_message']\n",
        "\n",
        "    for i in setup_1 + setup_2:      \n",
        "      j = i[:-4]\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Enron DATA - \", j, \"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      X_train, X_test, y_train, y_test = pd.read_csv(os.path.join(path, j+'_train.csv'))[main_labels],\\\n",
        "            pd.read_csv(os.path.join(path, j+'_test.csv'))[main_labels], pd.read_csv(os.path.join(path, j+'_train.csv'))[j],\\\n",
        "            pd.read_csv(os.path.join(path, j+'_test.csv'))[j]\n",
        "\n",
        "      \n",
        "      X_u = pd.read_csv(os.path.join(path, 'enron_unlabeled_2k.csv'))\n",
        "      y_u = pd.Series(np.array([-1 for i in range(X_u.shape[0])]))\n",
        "\n",
        "\n",
        "      to_be_used = 'full_message'\n",
        "      X_train = X_train[to_be_used]\n",
        "      X_test = X_test[to_be_used]\n",
        "      X_u = X_u[to_be_used]\n",
        "\n",
        "      tokenizer = Tokenizer(num_words=max_words)\n",
        "      tokenizer.fit_on_texts(np.concatenate((X_train, X_test, X_u)))\n",
        "      sequences = tokenizer.texts_to_sequences(np.concatenate((X_train, X_test, X_u)))\n",
        "      word_index = tokenizer.word_index                   \n",
        "      print(\"Found %s unique tokens.\" % len(word_index)) \n",
        "\n",
        "      data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "      X_train, X_test, X_u = data[:len(X_train)], data[len(X_train):len(X_train)+len(X_test)], data[len(X_train)+len(X_test):]\n",
        "\n",
        "      print(\"Supervised SGDClassifier on the training data:\")\n",
        "\n",
        "      model = KC_updated(model=lambda: create_model_cnn(word_index, 2), epochs=20, verbose=0)\n",
        "      pipeline = Pipeline([\n",
        "          # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          # ('tfidf', TfidfTransformer()),\n",
        "          ('clf', model),\n",
        "      ])\n",
        "          \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_train, y_train, X_test, y_test, thresh = None, kbest = None, label = j)\n",
        "      df_sgd_en = df_sgd_en.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for k in kbest_list:\n",
        "        print(\"---------------------------------Threshold = \", k,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        model = KC_updated(model=lambda: create_model_cnn(word_index, 2), epochs=20, verbose=0)\n",
        "        st_pipeline = Pipeline([\n",
        "            # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            # ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(model, criterion = 'k_best', k_best = k, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, np.concatenate((X_train, X_u)), np.concatenate((y_train, y_u)), X_test, y_test, thresh = None, kbest = k, label=j)\n",
        "        df_sgd_en = df_sgd_en.append(temp, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wn7Gu9p6tiD"
      },
      "source": [
        "df_sgd_en.to_excel('tf-cnn_enron_GloVE_non-trainable_840_with_roc.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLZRe4X4vD-4"
      },
      "source": [
        "# LSTM for Enron.\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "    df_sgd_en = pd.DataFrame()\n",
        "\n",
        "    path = 'data/enron/main'\n",
        "    main_labels = ['message', 'subject', 'body', 'full_message']\n",
        "\n",
        "    for i in setup_1 + setup_2:      \n",
        "      j = i[:-4]\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Enron DATA - \", j, \"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      X_train, X_test, y_train, y_test = pd.read_csv(os.path.join(path, j+'_train.csv'))[main_labels],\\\n",
        "            pd.read_csv(os.path.join(path, j+'_test.csv'))[main_labels], pd.read_csv(os.path.join(path, j+'_train.csv'))[j],\\\n",
        "            pd.read_csv(os.path.join(path, j+'_test.csv'))[j]\n",
        "\n",
        "      \n",
        "      X_u = pd.read_csv(os.path.join(path, 'enron_unlabeled_2k.csv'))\n",
        "      y_u = pd.Series(np.array([-1 for i in range(X_u.shape[0])]))\n",
        "\n",
        "\n",
        "      to_be_used = 'full_message'\n",
        "      X_train = X_train[to_be_used]\n",
        "      X_test = X_test[to_be_used]\n",
        "      X_u = X_u[to_be_used]\n",
        "\n",
        "      tokenizer = Tokenizer(num_words=max_words)\n",
        "      tokenizer.fit_on_texts(np.concatenate((X_train, X_test, X_u)))\n",
        "      sequences = tokenizer.texts_to_sequences(np.concatenate((X_train, X_test, X_u)))\n",
        "      word_index = tokenizer.word_index                   \n",
        "      print(\"Found %s unique tokens.\" % len(word_index)) \n",
        "\n",
        "      data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "      X_train, X_test, X_u = data[:len(X_train)], data[len(X_train):len(X_train)+len(X_test)], data[len(X_train)+len(X_test):]\n",
        "\n",
        "      print(\"Supervised SGDClassifier on the training data:\")\n",
        "\n",
        "      model = KerasClassifier(build_fn=lambda: create_model_lstm(word_index, 2), epochs=20, verbose=0)\n",
        "      pipeline = Pipeline([\n",
        "          # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          # ('tfidf', TfidfTransformer()),\n",
        "          ('clf', model),\n",
        "      ])\n",
        "          \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_train, y_train, X_test, y_test, thresh = None, kbest = None, label = j)\n",
        "      df_sgd_en = df_sgd_en.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        model = KerasClassifier(build_fn=lambda: create_model_lstm(word_index, 2), epochs=20, verbose=0)\n",
        "        st_pipeline = Pipeline([\n",
        "            # ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            # ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(model, criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, np.concatenate((X_train, X_u)), np.concatenate((y_train, y_u)), X_test, y_test, thresh = t, kbest = None, label=j)\n",
        "        df_sgd_en = df_sgd_en.append(temp, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJS_UWIE8STR"
      },
      "source": [
        "df_sgd_en.to_excel('tf-lstm_enron_GloVE_840_with_roc.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muADY5GP8cG1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}