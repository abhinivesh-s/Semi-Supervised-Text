{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semi-Supervised sklearn v4 MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPo0HGQUW0Mf4iLWdMkpWhk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinivesh-s/Semi-Supervised-Text/blob/main/Semi_Supervised_sklearn_v4_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_pmDcpcfyaM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data as Data\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa3rhWCfRR_v",
        "outputId": "53441714-fac5-49a2-a3fb-65b4427f5bf2"
      },
      "source": [
        "print(np.__version__)\n",
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.16.2\n",
            "0.24.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CINtLiQgPlK",
        "outputId": "34d08842-e0ba-4770-c4bf-1ac2d07abacb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "7epwVF9xRFok",
        "outputId": "47aba91c-ce71-4bf4-dc2e-2da6fc88fed8"
      },
      "source": [
        "!pip install numpy==1.16.2\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/e7/6c780e612d245cca62bc3ba8e263038f7c144a96a54f877f3714a0e8427e/numpy-1.16.2-cp37-cp37m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 331kB/s \n",
            "\u001b[31mERROR: xarray 0.18.2 has requirement numpy>=1.17, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyerfa 2.0.0 has requirement numpy>=1.17, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "Successfully installed numpy-1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueImsZ8KKQKo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "569c22f9-016b-450b-8c23-c8524bfe479b"
      },
      "source": [
        "!pip install --upgrade scikit-learn  # Do this to use sklearn SelfTrainingClassifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMut3gWbQUxB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feLsjKVQg-mk",
        "outputId": "e2a8bec4-867b-4ac0-ad4f-fb535158ce97"
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/MixText-master\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/MixText-master\n",
            " code\t     LICENSE\t\t\t'Running MixText v5 ISIS Data.ipynb'\n",
            " data\t     README.md\t\t\t'Semi-Supervised sklearn v2.ipynb'\n",
            " data_temp  'Running MixText v3.ipynb'\t word2vec_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J5BgqtbO9Q8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuaecprcLFwE",
        "outputId": "f07abea4-d51b-46a0-d5c8-6eef2aa0421b"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "data = fetch_20newsgroups(subset='train', categories=None)\n",
        "print(\"%d documents\" % len(data.filenames))\n",
        "print(\"%d categories\" % len(data.target_names))\n",
        "print()\n",
        "\n",
        "\n",
        "def eval_and_print_metrics_df(clf, X_train, y_train, X_test, y_test, thresh = None, kbest = None):\n",
        "\n",
        "    dict1 = {}\n",
        "\n",
        "\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    \n",
        "    dict1['Labeled'] = len(X_train) - sum(1 for x in y_train if x == -1)\n",
        "    dict1['UnLabeled'] = sum(1 for x in y_train if x == -1)\n",
        "\n",
        "\n",
        "    \n",
        "    # if sum(1 for x in y_train if x == -1) == 0:\n",
        "    #     dict1['type'] = 'Supervised'\n",
        "    # else:\n",
        "    #     dict1['type'] = 'Semi-Supervised'\n",
        "\n",
        "    dict1['Threshold'] = thresh\n",
        "    dict1['K-Best'] = kbest\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "    dict1['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "    return dict1\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11314 documents\n",
            "20 categories\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agoXVj3bbmKv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vADSNzoftiZ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad55b605-334d-4474-b0dd-371c85c888ac"
      },
      "source": [
        "# MLP for NG\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Parameters\n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=100,activation = 'relu',solver='adam',random_state=1,learning_rate_init=0.01,\n",
        "                  learning_rate='adaptive')\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_ng = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', MLPClassifier(**mlp_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_ng = df_mlp_ng.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(MLPClassifier(**mlp_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test, thresh = t, kbest = None)\n",
        "        df_mlp_ng = df_mlp_ng.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.650\n",
            "Accuracy Score:  0.6496995404736656\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3708 new labels.\n",
            "End of iteration 2, added 497 new labels.\n",
            "End of iteration 3, added 65 new labels.\n",
            "End of iteration 4, added 33 new labels.\n",
            "End of iteration 5, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.662\n",
            "Accuracy Score:  0.6620714033227288\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3314 new labels.\n",
            "End of iteration 2, added 757 new labels.\n",
            "End of iteration 3, added 120 new labels.\n",
            "End of iteration 4, added 39 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 24 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 8 new labels.\n",
            "End of iteration 9, added 12 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.689\n",
            "Accuracy Score:  0.688582537999293\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2956 new labels.\n",
            "End of iteration 2, added 766 new labels.\n",
            "End of iteration 3, added 244 new labels.\n",
            "End of iteration 4, added 90 new labels.\n",
            "End of iteration 5, added 51 new labels.\n",
            "End of iteration 6, added 106 new labels.\n",
            "End of iteration 7, added 17 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 9 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.703\n",
            "Accuracy Score:  0.7030752916224814\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2653 new labels.\n",
            "End of iteration 2, added 806 new labels.\n",
            "End of iteration 3, added 302 new labels.\n",
            "End of iteration 4, added 128 new labels.\n",
            "End of iteration 5, added 50 new labels.\n",
            "End of iteration 6, added 26 new labels.\n",
            "End of iteration 7, added 21 new labels.\n",
            "End of iteration 8, added 149 new labels.\n",
            "End of iteration 9, added 82 new labels.\n",
            "End of iteration 10, added 8 new labels.\n",
            "Micro-averaged F1 score on test set: 0.716\n",
            "Accuracy Score:  0.7161541180629197\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2310 new labels.\n",
            "End of iteration 2, added 776 new labels.\n",
            "End of iteration 3, added 313 new labels.\n",
            "End of iteration 4, added 158 new labels.\n",
            "End of iteration 5, added 272 new labels.\n",
            "End of iteration 6, added 71 new labels.\n",
            "End of iteration 7, added 34 new labels.\n",
            "End of iteration 8, added 148 new labels.\n",
            "End of iteration 9, added 38 new labels.\n",
            "End of iteration 10, added 13 new labels.\n",
            "Micro-averaged F1 score on test set: 0.708\n",
            "Accuracy Score:  0.7076705549664192\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.762\n",
            "Accuracy Score:  0.7624602332979852\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3917 new labels.\n",
            "End of iteration 2, added 317 new labels.\n",
            "End of iteration 3, added 59 new labels.\n",
            "End of iteration 4, added 7 new labels.\n",
            "End of iteration 5, added 5 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.767\n",
            "Accuracy Score:  0.7674089784376105\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3652 new labels.\n",
            "End of iteration 2, added 434 new labels.\n",
            "End of iteration 3, added 104 new labels.\n",
            "End of iteration 4, added 87 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.812\n",
            "Accuracy Score:  0.8119476846942383\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3379 new labels.\n",
            "End of iteration 2, added 545 new labels.\n",
            "End of iteration 3, added 153 new labels.\n",
            "End of iteration 4, added 44 new labels.\n",
            "End of iteration 5, added 19 new labels.\n",
            "End of iteration 6, added 95 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 11 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 31 new labels.\n",
            "Micro-averaged F1 score on test set: 0.807\n",
            "Accuracy Score:  0.8066454577589254\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3137 new labels.\n",
            "End of iteration 2, added 542 new labels.\n",
            "End of iteration 3, added 170 new labels.\n",
            "End of iteration 4, added 73 new labels.\n",
            "End of iteration 5, added 37 new labels.\n",
            "End of iteration 6, added 39 new labels.\n",
            "End of iteration 7, added 112 new labels.\n",
            "End of iteration 8, added 21 new labels.\n",
            "End of iteration 9, added 103 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.752\n",
            "Accuracy Score:  0.751502297631672\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2870 new labels.\n",
            "End of iteration 2, added 550 new labels.\n",
            "End of iteration 3, added 218 new labels.\n",
            "End of iteration 4, added 127 new labels.\n",
            "End of iteration 5, added 55 new labels.\n",
            "End of iteration 6, added 26 new labels.\n",
            "End of iteration 7, added 20 new labels.\n",
            "End of iteration 8, added 13 new labels.\n",
            "End of iteration 9, added 31 new labels.\n",
            "End of iteration 10, added 158 new labels.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Micro-averaged F1 score on test set: 0.818\n",
            "Accuracy Score:  0.8179568752209261\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Micro-averaged F1 score on test set: 0.784\n",
            "Accuracy Score:  0.7836691410392365\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b11619ab0a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSelfTrainingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmlp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'threshold'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         ])\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_and_print_metrics_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_20\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mX_u50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_u50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mdf_mlp_ng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_mlp_ng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a47757991f00>\u001b[0m in \u001b[0;36meval_and_print_metrics_df\u001b[0;34m(clf, X_train, y_train, X_test, y_test, thresh, kbest)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mdict1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'K-Best'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     print(\"Micro-averaged F1 score on test set: \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1203\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBxHOzCxtiV1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "7aee4d7d-df1b-451b-d6cd-f2ceafb40385"
      },
      "source": [
        "df_mlp_ng.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">869.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.649700</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.662071</td>\n",
              "      <td>0.688583</td>\n",
              "      <td>0.703075</td>\n",
              "      <td>0.716154</td>\n",
              "      <td>0.707671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1695.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.762460</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.767409</td>\n",
              "      <td>0.811948</td>\n",
              "      <td>0.806645</td>\n",
              "      <td>0.751502</td>\n",
              "      <td>0.817957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2541.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.783669</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Threshold               NaN       0.4       0.5       0.6       0.7       0.8\n",
              "Labeled UnLabeled                                                            \n",
              "869.0   0.0        0.649700       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.662071  0.688583  0.703075  0.716154  0.707671\n",
              "1695.0  0.0        0.762460       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.767409  0.811948  0.806645  0.751502  0.817957\n",
              "2541.0  0.0        0.783669       NaN       NaN       NaN       NaN       NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsh4Cw9jtiFh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7seg1TvztiBo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b75y3iSrth9q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIJQjxP7th5l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkTYOI_kth1W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cq0BfLFthxP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVwt8PKSthtE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yExt9FUithoz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5x5sAZ3thiM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEN92k3etheS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhBzPqdEfJ7e"
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/MixText-master\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMUWFZuielQN"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def train_val_split(labels, n_labeled_per_class, unlabeled_per_class, n_labels, seed=0):\n",
        "    \"\"\"Split the original training set into labeled training set, unlabeled training set, development set\n",
        "\n",
        "    Arguments:\n",
        "        labels {list} -- List of labeles for original training set\n",
        "        n_labeled_per_class {int} -- Number of labeled data per class\n",
        "        unlabeled_per_class {int} -- Number of unlabeled data per class\n",
        "        n_labels {int} -- The number of classes\n",
        "\n",
        "    Keyword Arguments:\n",
        "        seed {int} -- [random seed of np.shuffle] (default: {0})\n",
        "\n",
        "    Returns:\n",
        "        [list] -- idx for labeled training set, unlabeled training set, development set\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    labels = np.array(labels)\n",
        "    train_labeled_idxs = []\n",
        "    train_unlabeled_idxs = []\n",
        "    val_idxs = []\n",
        "\n",
        "    for i in range(n_labels):\n",
        "        idxs = np.where(labels == i)[0]\n",
        "        np.random.shuffle(idxs)\n",
        "        if n_labels == 2:\n",
        "            # IMDB\n",
        "            \n",
        "            \n",
        "            \n",
        "            n_unlabeled_per_class = unlabeled_per_class   #10, 100, 500, 1000, 2500\n",
        "            train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            train_unlabeled_idxs.extend(idxs[500: 500 + n_unlabeled_per_class])\n",
        "            val_idxs.extend(idxs[-2000:])\n",
        "        \n",
        "        \n",
        "        \n",
        "            # train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            # train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            # train_unlabeled_idxs.extend(\n",
        "            #     idxs[500: 500 + 5000])\n",
        "            # val_idxs.extend(idxs[-2000:])\n",
        "            \n",
        "\n",
        "    return train_labeled_idxs, train_unlabeled_idxs, val_idxs\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "#Removes Punctuations\n",
        "def remove_punctuations(data):\n",
        "    punct_tag=re.compile(r'[^\\w\\s]')\n",
        "    data=punct_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes HTML syntaxes\n",
        "def remove_html(data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes URL data\n",
        "def remove_url(data):\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def clean(train_df):\n",
        "    \n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_html(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_url(z))\n",
        "    # train_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\n",
        "    \n",
        "    train_df['review']=train_df['review'].apply(lambda z: word_tokenize(z))\n",
        "    \n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # train_df['review']=train_df['review'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "    train_df['review']=train_df['review'].apply(lambda x: ' '.join(x))\n",
        "    \n",
        "    return train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ8OCqGFirni"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Parameters\n",
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "# Supervised Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(**sdg_params)),\n",
        "])\n",
        "\n",
        "# # Supervised Pipeline\n",
        "# pipeline = Pipeline([\n",
        "#     ('vect', CountVectorizer(**vectorizer_params)),\n",
        "#     ('tfidf', TfidfTransformer()),\n",
        "#     ('clf', SVC(probability=True, gamma=\"auto\")),\n",
        "# ])\n",
        "\n",
        "# SelfTraining Pipeline\n",
        "st_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True)),\n",
        "])\n",
        "\n",
        "# # SelfTraining Pipeline\n",
        "# st_pipeline = Pipeline([\n",
        "#     ('vect', CountVectorizer(**vectorizer_params)),\n",
        "#     ('tfidf', TfidfTransformer()),\n",
        "#     ('clf', SelfTrainingClassifier(SVC(probability=True, gamma=\"auto\") )),\n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# LabelSpreading Pipeline\n",
        "ls_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    # LabelSpreading does not support dense matrices\n",
        "    ('todense', FunctionTransformer(lambda x: x.todense())),\n",
        "    ('clf', LabelSpreading()),\n",
        "])\n",
        "\n",
        "\n",
        "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # if -1 in y_train:\n",
        "    #   print(\"Y-PRED-PROBA\", clf.predict_proba(X_train))\n",
        "\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "def eval_and_print_metrics_df(clf, X_train, y_train, X_test, y_test, thresh = None, kbest = None):\n",
        "\n",
        "    dict1 = {}\n",
        "\n",
        "\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    \n",
        "    dict1['Labeled'] = len(X_train) - sum(1 for x in y_train if x == -1)\n",
        "    dict1['UnLabeled'] = sum(1 for x in y_train if x == -1)\n",
        "\n",
        "\n",
        "    \n",
        "    # if sum(1 for x in y_train if x == -1) == 0:\n",
        "    #     dict1['type'] = 'Supervised'\n",
        "    # else:\n",
        "    #     dict1['type'] = 'Semi-Supervised'\n",
        "\n",
        "    dict1['Threshold'] = thresh\n",
        "    dict1['K-Best'] = kbest\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "    dict1['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "    return dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJbPC72BjeZ4"
      },
      "source": [
        "# MLP for IMDB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=50,activation = 'relu',solver='adam',random_state=1)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', MLPClassifier(**mlp_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(MLPClassifier(**mlp_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03U0nK6QjeWW"
      },
      "source": [
        "df_mlp_imdb_2k.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}